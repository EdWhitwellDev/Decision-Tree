I have attempted to "solve" the titanic problem using Decision trees
I didn't do much research into how they worked and tried to learn by developing them
I started by trying a few simple things, like grouping by a hierarchy of attributes,
Then i hard coded splits/predictions instead of using a machine learning technique

The first Tree i made, doesn't use gini or entropy, not because i used something better but because i didn't know what it was, it only did binary splits, and determined the critical value by trying different continuous values until the ratio of suvived and died got worse. a forest of 25 got 75-76% accuracy.

the next used gini and only categorical attributes, it got to around 77% again with 25 trees

the last one i am still working on. instead of only passing split data down the branches as i did with the other one, if the split does not improve the accuracy for a catagory, the complete data is is passed down its branch, however the branch then decides the next attribute based on how well it predicts the previously poorly predicted catagory. I am still developing this one, however it gets 79.2% accuracy with only 1 tree, i hope to get it to 80% with only one tree, but failing that i will use a forest with some kind of boosting.   

I moved on from the the Tree i was working on when i emailed and made a new model inspired by the hard coded one i made or rather the process i went through to make it. In the commit message it currently says it does it without a forrest  which i kind of true, however it does make multiple predictions. It works by ordering each attribute by its ability to predict survival, and takes only the best ones required to predict every Passenger. Some catagories are considered solid if they meet a standard in which case they are set to make a prediction, the ones that are still good but not quite good enough have the data split so it is only of passengers of that specific catagory, then the process is repeated. To then predict for unseen passengers, at each level the passenger goes down every branch it has the correct catagory for, at each stage the prediction is voted on, the weight of each vote is based of its accuracy, though solid nodes have a multiplier. That is my very rough explanation, it makes 80% which i think is condidered very good, its probably easier to look at the code than to read my explanation even though it isn't currently commented.  
 